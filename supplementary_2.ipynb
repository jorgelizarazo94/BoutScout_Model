{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "<img src=\"assets/logo.png\" width=\"300px\">\n",
    "\n",
    "# Supplementary 2 - Cross-Validation and Model Stability Assessment\n",
    "**BoutScout: A Deep Learning Framework for Automatic Detection of Incubation Events in Avian Nests Using Temperature Time Series**\n",
    "\n",
    "Author: [Jorge Lizarazo](https://www.researchgate.net/profile/Jorge-Lizarazo-Borrero?ev=hdr_xprf)\n",
    "\n",
    "\n",
    "This supplementary notebook presents the cross-validation procedure used to assess the generalization and robustness of the BoutScout model. Using a 5-fold StratifiedKFold approach, we evaluated model performance across multiple splits while ensuring balanced class distributions per fold. For each fold, the BiLSTM model was reinitialized and trained from scratch for 50 epochs, using identical architecture and training settings.\n",
    "\n",
    "In addition to fold-specific plots, this notebook includes mean loss curves, average precision-recall curves with standard deviation bands, and normalized confusion matrices across folds to illustrate performance consistency.\n",
    "\n",
    "**Year:** 2025\n"
   ],
   "id": "c1eb9bead16e19ac"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:06:43.496004Z",
     "start_time": "2025-06-15T01:06:43.475705Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import numpy as np\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import json\n",
    "import ast\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from scipy.interpolate import interp1d\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "from sklearn.preprocessing import label_binarize"
   ],
   "id": "3deec47109d6f219",
   "outputs": [],
   "execution_count": 40
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:08:50.966192Z",
     "start_time": "2025-06-15T01:08:50.950568Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#print(torch.__version__)\n",
    "#print(torch.cuda.is_available())\n",
    "#print(torch.cuda.get_device_name(0) if torch.cuda.is_available() else \"No GPU detected\")"
   ],
   "id": "3f5a73f9f1896670",
   "outputs": [],
   "execution_count": 42
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:08:55.458835Z",
     "start_time": "2025-06-15T01:08:55.347793Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Kargar arrays\n",
    "X_array = np.load(\"X_array_cleaned.npy\", allow_pickle=True)\n",
    "y_array = np.load(\"y_array_cleaned.npy\", allow_pickle=True)\n",
    "\n",
    "# Kargar clases del LabelEncoder\n",
    "le = LabelEncoder()\n",
    "le.classes_ = np.load(\"label_classes.npy\", allow_pickle=True)\n",
    "\n",
    "# Definir Dataset\n",
    "class NestEventDataset(Dataset):\n",
    "    def __init__(self, X_array, y_array):\n",
    "        self.X_array = X_array\n",
    "        self.y_array = y_array\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.X_array)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        X = self.X_array[idx].astype(np.float32)\n",
    "        y = self.y_array[idx].astype(np.int64)\n",
    "        return torch.tensor(X), torch.tensor(y)\n",
    "\n",
    "# Crear el dataset\n",
    "dataset = NestEventDataset(X_array, y_array)"
   ],
   "id": "864d93ae222db96a",
   "outputs": [],
   "execution_count": 43
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "",
   "id": "107b582d1d547151"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:08:57.027087Z",
     "start_time": "2025-06-15T01:08:57.004815Z"
    }
   },
   "cell_type": "code",
   "source": [
    "\n",
    "idx = random.randint(0, len(dataset) - 1)\n",
    "X_sample, y_sample = dataset[idx]\n",
    "X_np = X_sample.numpy()\n",
    "y_np = y_sample.numpy()"
   ],
   "id": "b5cd3b3c7ecd7e96",
   "outputs": [],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T00:43:14.335716Z",
     "start_time": "2025-06-15T00:43:14.320076Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "36920e4d0b7c1026",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:08:59.042508Z",
     "start_time": "2025-06-15T01:08:59.026669Z"
    }
   },
   "cell_type": "code",
   "source": "print(f\"Numero total de dias (entradas): {len(y_array)}\")",
   "id": "9de28afa4d308bbb",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número total de días (entradas): 2232\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:00.442909Z",
     "start_time": "2025-06-15T01:09:00.427337Z"
    }
   },
   "cell_type": "code",
   "source": [
    "idx = random.randint(0, len(X_array) - 1)\n",
    "X_sample = X_array[idx].astype(np.float32)\n",
    "y_sample = y_array[idx].astype(np.int64)\n",
    "\n",
    "X_np = X_sample\n",
    "y_np = y_sample"
   ],
   "id": "1ee5389ad4350b36",
   "outputs": [],
   "execution_count": 46
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:00.961927Z",
     "start_time": "2025-06-15T01:09:00.945937Z"
    }
   },
   "cell_type": "code",
   "source": "y_np",
   "id": "dce6dfaeed4ac706",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, ..., 0, 0, 0], dtype=int64)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:02.105999Z",
     "start_time": "2025-06-15T01:09:02.090388Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class BiLSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(BiLSTMModel, self).__init__()\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=input_size,\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True,\n",
    "            bidirectional=True\n",
    "        )\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        lstm_out, _ = self.lstm(x)\n",
    "        out = self.fc(lstm_out)\n",
    "        return out"
   ],
   "id": "8b9ccc62d6cd5e6d",
   "outputs": [],
   "execution_count": 48
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:03.431284Z",
     "start_time": "2025-06-15T01:09:03.415611Z"
    }
   },
   "cell_type": "code",
   "source": [
    "input_size = X_array[0].shape[1]  # número de features por minuto (ej: temperatura, ambiente, etc.)\n",
    "hidden_size = 64\n",
    "num_layers = 2\n",
    "num_classes = 3  # ['Error', 'Nocturnal', 'Off', 'On'], aunque eliminaste \"Error\", mantenemos por consistencia\n",
    "\n",
    "model = BiLSTMModel(input_size, hidden_size, num_layers, num_classes)"
   ],
   "id": "ae05ac5e0e80ae3b",
   "outputs": [],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:10.564978Z",
     "start_time": "2025-06-15T01:09:10.533775Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "7972803c5b1ba10d",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:04:23.981995Z",
     "start_time": "2025-06-15T01:04:23.966402Z"
    }
   },
   "cell_type": "code",
   "source": "",
   "id": "51dbac04237a3016",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:12.749315Z",
     "start_time": "2025-06-15T01:09:12.724577Z"
    }
   },
   "cell_type": "code",
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)"
   ],
   "id": "f2d4252e58c43d1e",
   "outputs": [],
   "execution_count": 51
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:14.218801Z",
     "start_time": "2025-06-15T01:09:14.203246Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "import torch.nn as nn\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import classification_report\n",
    "import os"
   ],
   "id": "c5cb6055b989535c",
   "outputs": [],
   "execution_count": 52
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "We set up directories to save confusion matrices, precision-recall curves, and loss plots.",
   "id": "e403b29093a4c149"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:16.623090Z",
     "start_time": "2025-06-15T01:09:16.607614Z"
    }
   },
   "cell_type": "code",
   "source": [
    "os.makedirs(\"figures/confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(\"figures/precision_recall\", exist_ok=True)\n",
    "os.makedirs(\"figures/loss_curves\", exist_ok=True)"
   ],
   "id": "abc86971fed1aeb4",
   "outputs": [],
   "execution_count": 53
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Definning dominant class per day for stratification, create result folders, and set up 5-fold StratifiedKFold.",
   "id": "c34fe1e1115f76b1"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T01:09:18.387769Z",
     "start_time": "2025-06-15T01:09:18.359284Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Etiqueta por día dominante\n",
    "# Crear carpetas de salida si no existen\n",
    "os.makedirs(\"resultados_folds\", exist_ok=True)\n",
    "os.makedirs(\"modelos_folds\", exist_ok=True)\n",
    "\n",
    "mejor_f1 = 0\n",
    "mejor_fold = -1\n",
    "y_mode = [np.bincount(y).argmax() for y in y_array]\n",
    "\n",
    "\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "fold_scores = []\n",
    "ruta_mejor_modelo = \"modelos/modelo_bilstm_cv_mejor_cross.pth\""
   ],
   "id": "1748df4ed1dce30",
   "outputs": [],
   "execution_count": 54
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## 5-Fold Cross-Validation Training Loop\n",
    "We perform 5-fold cross-validation using a StratifiedKFold split based on each day's dominant label.\n",
    "For each fold:\n",
    "- The BiLSTM model is reinitialized and trained for 50 epochs.\n",
    "- Training and validation losses are computed.\n",
    "- Predictions are collected to compute F1-score, classification reports, and confusion matrices.\n",
    "- Training losses and predictions are saved for later analysis.\n",
    "- The model with the best macro F1-score across folds is saved for deployment."
   ],
   "id": "d4e5dc147e4bada7"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:41:51.888726Z",
     "start_time": "2025-06-15T05:15:36.574834Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_array, y_mode), 1):\n",
    "    print(f\"\\n=== Fold {fold}/5 ===\")\n",
    "\n",
    "    X_train = [X_array[i] for i in train_idx]\n",
    "    y_train = [y_array[i] for i in train_idx]\n",
    "    X_val = [X_array[i] for i in val_idx]\n",
    "    y_val = [y_array[i] for i in val_idx]\n",
    "\n",
    "    train_ds = NestEventDataset(X_train, y_train)\n",
    "    val_ds = NestEventDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_ds, batch_size=16, shuffle=True)\n",
    "    val_loader = DataLoader(val_ds, batch_size=16, shuffle=False)\n",
    "\n",
    "    model = BiLSTMModel(input_size=X_array[0].shape[1], hidden_size=64, num_layers=2, num_classes=3).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    train_losses = []\n",
    "\n",
    "    for epoch in range(50):\n",
    "        model.train()\n",
    "        for X_batch, y_batch in train_loader:\n",
    "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(X_batch)\n",
    "            loss = criterion(outputs.view(-1, outputs.shape[-1]), y_batch.view(-1))\n",
    "            loss.backward()\n",
    "            train_losses.append(loss.item())\n",
    "            optimizer.step()\n",
    "\n",
    "    # Evaluación\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "    with torch.no_grad():\n",
    "        for X_batch, y_batch in val_loader:\n",
    "            X_batch = X_batch.to(device)\n",
    "            outputs = model(X_batch)\n",
    "            preds = torch.argmax(outputs, dim=2).view(-1).cpu().numpy()\n",
    "            labels = y_batch.view(-1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels)\n",
    "\n",
    "        np.save(f\"resultados_folds/fold_{fold}_train_losses.npy\", np.array(train_losses))\n",
    "        np.save(f\"resultados_folds/fold_{fold}_labels.npy\", np.array(all_labels))\n",
    "        np.save(f\"resultados_folds/fold_{fold}_preds.npy\", np.array(all_preds))\n",
    "\n",
    "    report = classification_report(all_labels, all_preds, target_names=['Nocturnal', 'Off', 'On'], zero_division=0, output_dict=True)\n",
    "    f1_macro = report['macro avg']['f1-score']\n",
    "    fold_scores.append(f1_macro)\n",
    "\n",
    "    print(f\"F1 macro (fold {fold}): {f1_macro:.4f}\")\n",
    "\n",
    "    # === Guardar resultados y modelo de este fold ===\n",
    "    with open(f\"resultados_folds/fold_{fold}_reporte.json\", \"w\") as f:\n",
    "        json.dump(report, f, indent=4)\n",
    "\n",
    "    torch.save(model.state_dict(), f\"modelos_folds/modelo_fold_{fold}.pth\")\n",
    "\n",
    "    if f1_macro > mejor_f1:\n",
    "        mejor_f1 = f1_macro\n",
    "        mejor_fold = fold\n",
    "        torch.save(model.state_dict(), ruta_mejor_modelo)\n",
    "        print(f\"✅ Guardado mejor modelo (F1: {mejor_f1:.4f}) en fold {mejor_fold}\")\n",
    "\n",
    "print(f\"\\n=== Promedio F1 macro en validación cruzada: {np.mean(fold_scores):.4f} ===\")\n",
    "print(f\"🏆 Mejor modelo guardado fue del fold {mejor_fold} con F1 macro = {mejor_f1:.4f}\")"
   ],
   "id": "562b1c89256608b9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Fold 1/5 ===\n",
      "F1 macro (fold 1): 0.9089\n",
      "\n",
      "=== Fold 2/5 ===\n",
      "F1 macro (fold 2): 0.9235\n",
      "\n",
      "=== Fold 3/5 ===\n",
      "F1 macro (fold 3): 0.9433\n",
      "\n",
      "=== Fold 4/5 ===\n",
      "F1 macro (fold 4): 0.9577\n",
      "✅ Guardado mejor modelo (F1: 0.9577) en fold 4\n",
      "\n",
      "=== Fold 5/5 ===\n",
      "F1 macro (fold 5): 0.9520\n",
      "\n",
      "=== Promedio F1 macro en validación cruzada: 0.9419 ===\n",
      "🏆 Mejor modelo guardado fue del fold 4 con F1 macro = 0.9577\n"
     ]
    }
   ],
   "execution_count": 57
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "58f07f0ba62057d1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Per-fold visualizations\n",
    "We generate and save key diagnostic plots for each fold:\n",
    "- **Training loss curves** to monitor convergence.\n",
    "- **Confusion matrices** to assess class-specific performance.\n",
    "- **Precision–recall curves** with average precision (AP) per class."
   ],
   "id": "6ecabeb5621e6566"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-06-15T08:41:52.278609Z",
     "start_time": "2025-06-15T08:41:51.888726Z"
    }
   },
   "cell_type": "code",
   "source": [
    "folds = [1, 2, 3, 4, 5]\n",
    "class_names = ['Nocturnal', 'Off', 'On']\n",
    "os.makedirs(\"figures/confusion_matrices\", exist_ok=True)\n",
    "os.makedirs(\"figures/precision_recall\", exist_ok=True)\n",
    "os.makedirs(\"figures/loss_curves\", exist_ok=True)\n",
    "\n",
    "for fold in folds:\n",
    "    losses = np.load(f\"resultados_folds/fold_{fold}_train_losses.npy\")\n",
    "    labels = np.load(f\"resultados_folds/fold_{fold}_labels.npy\")\n",
    "    preds = np.load(f\"resultados_folds/fold_{fold}_preds.npy\")\n",
    "\n",
    "    # 1. Loss Curve\n",
    "    plt.figure()\n",
    "    plt.plot(losses, label='Training Loss', color='blue')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title(f'Training Loss - Fold {fold}')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/loss_curves/fold_{fold}_loss.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 2. Confusion Matrix\n",
    "    cm = confusion_matrix(labels, preds)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "    fig, ax = plt.subplots(figsize=(6, 6))\n",
    "    disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "    plt.title(f'Confusion Matrix - Fold {fold}')\n",
    "    plt.savefig(f\"figures/confusion_matrices/fold_{fold}_confusion_matrix.png\")\n",
    "    plt.close()\n",
    "\n",
    "    # 3. Precision-Recall Curve\n",
    "    y_true_bin = label_binarize(labels, classes=[0, 1, 2])\n",
    "    y_score_bin = label_binarize(preds, classes=[0, 1, 2])\n",
    "    plt.figure(figsize=(7, 5))\n",
    "    for i, class_name in enumerate(class_names):\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin[:, i], y_score_bin[:, i])\n",
    "        ap = average_precision_score(y_true_bin[:, i], y_score_bin[:, i])\n",
    "        plt.plot(recall, precision, lw=2, label=f'{class_name} (AP = {ap:.2f})')\n",
    "\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(f'Precision-Recall Curve - Fold {fold}')\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"figures/precision_recall/fold_{fold}_pr_curve.png\")\n",
    "    plt.close()"
   ],
   "id": "2920a2a4419f91b7",
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'label_binarize' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001B[1;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[1;31mNameError\u001B[0m                                 Traceback (most recent call last)",
      "Cell \u001B[1;32mIn[58], line 33\u001B[0m\n\u001B[0;32m     30\u001B[0m plt\u001B[38;5;241m.\u001B[39mclose()\n\u001B[0;32m     32\u001B[0m \u001B[38;5;66;03m# 3. Precision-Recall Curve\u001B[39;00m\n\u001B[1;32m---> 33\u001B[0m y_true_bin \u001B[38;5;241m=\u001B[39m \u001B[43mlabel_binarize\u001B[49m(labels, classes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m     34\u001B[0m y_score_bin \u001B[38;5;241m=\u001B[39m label_binarize(preds, classes\u001B[38;5;241m=\u001B[39m[\u001B[38;5;241m0\u001B[39m, \u001B[38;5;241m1\u001B[39m, \u001B[38;5;241m2\u001B[39m])\n\u001B[0;32m     35\u001B[0m plt\u001B[38;5;241m.\u001B[39mfigure(figsize\u001B[38;5;241m=\u001B[39m(\u001B[38;5;241m7\u001B[39m, \u001B[38;5;241m5\u001B[39m))\n",
      "\u001B[1;31mNameError\u001B[0m: name 'label_binarize' is not defined"
     ]
    }
   ],
   "execution_count": 58
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mean loss curve across folds\n",
    "To evaluate the overall training behavior of the model, we aggregate the training loss curves from all five cross-validation folds. We compute the mean loss per epoch and the standard deviation across folds to assess convergence consistency. The resulting plot shows the average training trajectory with a shaded area representing ±1 standard deviation, helping visualize how stable the learning process is across splits."
   ],
   "id": "78567e0bd0120a3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "all_losses = []\n",
    "\n",
    "# Recolectar todas las curvas de pérdida\n",
    "for fold in folds:\n",
    "    losses = np.load(f\"resultados_folds/fold_{fold}_train_losses.npy\")\n",
    "    all_losses.append(losses)\n",
    "\n",
    "# Convertir a array para calcular promedio y desviación estándar\n",
    "all_losses = np.array(all_losses)\n",
    "mean_loss = all_losses.mean(axis=0)\n",
    "std_loss = all_losses.std(axis=0)\n",
    "\n",
    "# Plot\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(mean_loss, label='Mean Loss', color='#707070', linewidth=2)\n",
    "plt.fill_between(range(len(mean_loss)), mean_loss - std_loss, mean_loss + std_loss,\n",
    "                 color='#C7C7C7', alpha=0.3, label='±1 Std. Dev')\n",
    "\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "#plt.title('Training Loss per Epoch\\nMean and Std. Deviation across Folds')\n",
    "plt.grid(False)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"figures/loss_curves/mean_loss_curve_only.png\")\n",
    "plt.show()\n"
   ],
   "id": "df40b8e79f6e5b13",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Mean precision–recall curves by class\n",
    "We compute average precision–recall (PR) curves across folds for each behavioral class (`Nocturnal`, `Off`, and `On`).\n",
    "To ensure comparability, precision values are interpolated over a fixed recall grid from 0 to 1.\n",
    "We then calculate the mean and standard deviation of precision across folds, and plot shaded bands to represent variability.\n",
    "This visualization allows us to evaluate the model’s ability to distinguish each class consistently across different data splits."
   ],
   "id": "6a0929e60c7d34d7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "class_names = ['Nocturnal', 'Off', 'On']\n",
    "\n",
    "# Colores personalizados por clase\n",
    "custom_colors = {\n",
    "    'On': '#E28342',\n",
    "    'Off': '#535AA6',\n",
    "    'Nocturnal': '#333E48'\n",
    "}\n",
    "\n",
    "# Puntos comunes de recall (de 0 a 1)\n",
    "recall_grid = np.linspace(0, 1, 100)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "for class_idx, class_name in enumerate(class_names):\n",
    "    interpolated_precisions = []\n",
    "\n",
    "    for fold in folds:\n",
    "        labels = np.load(f\"resultados_folds/fold_{fold}_labels.npy\")\n",
    "        preds = np.load(f\"resultados_folds/fold_{fold}_preds.npy\")\n",
    "\n",
    "        y_true_bin = label_binarize(labels, classes=[0, 1, 2])[:, class_idx]\n",
    "        y_pred_bin = label_binarize(preds, classes=[0, 1, 2])[:, class_idx]\n",
    "\n",
    "        precision, recall, _ = precision_recall_curve(y_true_bin, y_pred_bin)\n",
    "\n",
    "        # Interpolar precisión en los mismos valores de recall\n",
    "        interp = interp1d(recall, precision, kind='linear', bounds_error=False,\n",
    "                          fill_value=(precision[0], precision[-1]))\n",
    "        interp_prec = interp(recall_grid)\n",
    "        interpolated_precisions.append(interp_prec)\n",
    "\n",
    "    interpolated_precisions = np.array(interpolated_precisions)\n",
    "    mean_precision = interpolated_precisions.mean(axis=0)\n",
    "    std_precision = interpolated_precisions.std(axis=0)\n",
    "\n",
    "    color = custom_colors[class_name]\n",
    "\n",
    "    # Plot media + área de desviación estándar\n",
    "    plt.plot(recall_grid, mean_precision, label=class_name, linewidth=2, color=color)\n",
    "    plt.fill_between(recall_grid, mean_precision - std_precision, mean_precision + std_precision,\n",
    "                     alpha=0.2, color=color)\n",
    "\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "#plt.title('Mean Precision-Recall Curves with Std. Deviation')\n",
    "plt.legend(title=\"Class\")\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "#plt.savefig(\"figures/precision_recall/mean_PR_curves_all_classes.png\")\n",
    "plt.show()"
   ],
   "id": "ddb2cee2b29528bd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "### Normalized confusion matrices across folds\n",
    "We visualize the class-wise prediction accuracy for each fold using confusion matrices normalized by row (i.e., per true class).\n",
    "Each matrix shows the proportion of correctly and incorrectly classified time steps per behavioral classes.\n",
    "Grayscale shading is used to highlight prediction strength, with values displayed as percentages.\n",
    "This view allows for quick comparison of classification performance consistency across folds and highlights typical confusion patterns between similar behaviors."
   ],
   "id": "ae5a2f5c78ae9a2"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "from matplotlib.colors import ListedColormap",
   "id": "8fbfb3edf4eee538"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "plt.style.use(\"seaborn-v0_8-whitegrid\")\n",
    "folds = [1, 2, 3, 4, 5]\n",
    "class_names = ['Nocturnal', 'Off', 'On']\n",
    "\n",
    "\n",
    "# Crear paleta de grises personalizada del claro al oscuro\n",
    "custom_greys = ListedColormap([\"#EAEAEA\", \"#C7C7C7\", \"#A0A0A0\", \"#707070\"])\n",
    "\n",
    "fig, axes = plt.subplots(1, 5, figsize=(14, 4))\n",
    "\n",
    "for i, fold in enumerate(folds):\n",
    "    labels = np.load(f\"resultados_folds/fold_{fold}_labels.npy\")\n",
    "    preds = np.load(f\"resultados_folds/fold_{fold}_preds.npy\")\n",
    "\n",
    "    # Normalizar por fila para obtener porcentajes\n",
    "    cm = confusion_matrix(labels, preds, normalize='true') * 100\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
    "\n",
    "    disp.plot(ax=axes[i], cmap=custom_greys, colorbar=False)  # ✅ Colores grises\n",
    "    axes[i].set_title(f'Fold {fold}', fontsize=16)\n",
    "    axes[i].set_xlabel('Predicted', fontsize=13)\n",
    "    axes[i].set_ylabel('True', fontsize=13)\n",
    "    axes[i].grid(False)  # ✅ Eliminar cuadrícula\n",
    "\n",
    "    for text in axes[i].texts:\n",
    "        val = float(text.get_text())\n",
    "        text.set_text(f'{val:.1f}%')\n",
    "        text.set_fontsize(10)\n",
    "\n",
    "#plt.suptitle('Normalized Confusion Matrices (% per row)', fontsize=16)\n",
    "plt.tight_layout(rect=[0, 0, 1, 0.95])\n",
    "#plt.savefig(\"figures/Normalized_Confusion_Matrice.png\", dpi=300)\n",
    "plt.show()"
   ],
   "id": "cd2f382f10881243"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "Please go to the next, [Supplementary 3](https://github.com/jorgelizarazo94/BoutScout_Model/blob/05fdad347af45f8804484a6c9b0b3592751f4a83/supplementary_3.ipynb)",
   "id": "3104ee99fdfabcee"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
